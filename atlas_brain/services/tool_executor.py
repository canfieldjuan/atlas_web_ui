"""
Tool execution service for LLM tool calling.

Handles the tool calling loop:
1. Call LLM with tool schemas
2. Parse tool calls from response
3. Execute tools via registry
4. Format tool results
5. Call LLM again for final response
"""

import asyncio
import json
import logging
import re
from typing import Any

from ..tools import tool_registry
from ..tools.base import ToolResult
from .protocols import Message

logger = logging.getLogger("atlas.services.tool_executor")

MAX_TOOL_ITERATIONS = 3

# Response returned when the LLM produces no text and no tool calls.
# Exported so callers can detect this fallback and retry with plain chat.
EMPTY_RESPONSE_FALLBACK = "Sorry, I wasn't able to process that."

# Priority tools for LLM tool calling (reduces model confusion)
PRIORITY_TOOL_NAMES = [
    "get_time", "get_weather", "get_calendar", "get_location",
    "set_reminder", "list_reminders", "send_notification",
    "send_email", "check_availability", "book_appointment",
    "cancel_appointment", "reschedule_appointment",
    # MCP-provided tools (read-heavy set for general conversation)
    "search_contacts", "get_contact", "get_customer_context",
    "list_events", "get_event", "find_free_slots", "create_event",
    "list_inbox", "get_message", "search_inbox", "list_folders",
]

# Pattern to match text-based tool calls
# Format 1: <function=tool_name>json_args</function>
TEXT_TOOL_PATTERN = re.compile(
    r"<function=(\w+)>(.*?)</function>",
    re.DOTALL
)

# Pattern to extract parameter tags
PARAM_PATTERN = re.compile(
    r"<parameter=(\w+)>\s*(.*?)\s*</parameter>",
    re.DOTALL
)


def _strip_tool_xml(text: str) -> str:
    """Remove tool call XML artifacts and LLM thinking tags from a response."""
    text = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL)
    text = re.sub(r"</?tool_call>", "", text)
    text = TEXT_TOOL_PATTERN.sub("", text)
    return text.strip()


def parse_text_tool_calls(content: str) -> list[dict]:
    """Parse text-based tool calls from LLM response."""
    tool_calls = []
    for match in TEXT_TOOL_PATTERN.finditer(content):
        tool_name = match.group(1)
        inner_content = match.group(2).strip()
        args = {}

        # Try to parse as JSON first
        if inner_content and not inner_content.startswith("<"):
            try:
                args = json.loads(inner_content)
            except json.JSONDecodeError:
                pass

        # Try to parse parameter tags
        if not args:
            for param_match in PARAM_PATTERN.finditer(inner_content):
                param_name = param_match.group(1)
                param_value = param_match.group(2).strip()
                args[param_name] = param_value

        tool_calls.append({
            "function": {
                "name": tool_name,
                "arguments": args,
            }
        })
    return tool_calls


def _to_int_or_none(value: Any) -> int | None:
    """Coerce value to int when possible."""
    try:
        if value is None:
            return None
        return int(value)
    except (TypeError, ValueError):
        return None


def _to_float_or_none(value: Any) -> float | None:
    """Coerce value to float when possible."""
    try:
        if value is None:
            return None
        return float(value)
    except (TypeError, ValueError):
        return None


def _extract_llm_meta(result: dict[str, Any]) -> dict[str, Any]:
    """Extract normalized LLM metadata from one model call result."""
    provider_request_id = result.get("request_id") or result.get("id")
    if not isinstance(provider_request_id, str) or not provider_request_id.strip():
        provider_request_id = None
    return {
        "input_tokens": _to_int_or_none(result.get("prompt_eval_count")),
        "output_tokens": _to_int_or_none(result.get("eval_count")),
        "prompt_eval_duration_ms": _to_float_or_none(result.get("prompt_eval_duration_ms")),
        "eval_duration_ms": _to_float_or_none(result.get("eval_duration_ms")),
        "total_duration_ms": _to_float_or_none(result.get("total_duration_ms")),
        "provider_request_id": provider_request_id,
        "has_llm_response": True,
    }


def _merge_llm_meta(aggregate: dict[str, Any] | None, update: dict[str, Any]) -> dict[str, Any]:
    """Merge one call's LLM metadata into an aggregate metadata dict."""
    if aggregate is None:
        aggregate = {
            "input_tokens": 0,
            "output_tokens": 0,
            "prompt_eval_duration_ms": 0.0,
            "eval_duration_ms": 0.0,
            "total_duration_ms": 0.0,
            "provider_request_id": None,
            "has_llm_response": False,
        }

    input_tokens = _to_int_or_none(update.get("input_tokens"))
    if input_tokens is not None:
        aggregate["input_tokens"] = int(aggregate["input_tokens"]) + input_tokens

    output_tokens = _to_int_or_none(update.get("output_tokens"))
    if output_tokens is not None:
        aggregate["output_tokens"] = int(aggregate["output_tokens"]) + output_tokens

    prompt_eval_ms = _to_float_or_none(update.get("prompt_eval_duration_ms"))
    if prompt_eval_ms is not None:
        aggregate["prompt_eval_duration_ms"] = float(aggregate["prompt_eval_duration_ms"]) + prompt_eval_ms

    eval_ms = _to_float_or_none(update.get("eval_duration_ms"))
    if eval_ms is not None:
        aggregate["eval_duration_ms"] = float(aggregate["eval_duration_ms"]) + eval_ms

    total_ms = _to_float_or_none(update.get("total_duration_ms"))
    if total_ms is not None:
        aggregate["total_duration_ms"] = float(aggregate["total_duration_ms"]) + total_ms

    provider_request_id = update.get("provider_request_id")
    if isinstance(provider_request_id, str) and provider_request_id.strip():
        aggregate["provider_request_id"] = provider_request_id

    aggregate["has_llm_response"] = bool(
        aggregate.get("has_llm_response")
        or update.get("has_llm_response")
    )
    return aggregate


def _finalize_llm_meta(aggregate: dict[str, Any] | None) -> dict[str, Any]:
    """Return stable llm_meta shape for callers."""
    if aggregate is None:
        return {
            "input_tokens": None,
            "output_tokens": None,
            "prompt_eval_duration_ms": None,
            "eval_duration_ms": None,
            "total_duration_ms": None,
            "provider_request_id": None,
            "has_llm_response": False,
        }

    return {
        "input_tokens": _to_int_or_none(aggregate.get("input_tokens")),
        "output_tokens": _to_int_or_none(aggregate.get("output_tokens")),
        "prompt_eval_duration_ms": _to_float_or_none(aggregate.get("prompt_eval_duration_ms")),
        "eval_duration_ms": _to_float_or_none(aggregate.get("eval_duration_ms")),
        "total_duration_ms": _to_float_or_none(aggregate.get("total_duration_ms")),
        "provider_request_id": aggregate.get("provider_request_id"),
        "has_llm_response": bool(aggregate.get("has_llm_response")),
    }


async def execute_with_tools(
    llm,
    messages: list[Message],
    max_tokens: int = 256,
    temperature: float = 0.7,
    target_tool: str | None = None,
    tools_override: list[dict] | None = None,
) -> dict[str, Any]:
    """
    Execute LLM query with tool calling loop.

    Args:
        llm: LLM service with chat_with_tools method
        messages: Initial message list
        max_tokens: Max tokens for LLM response
        temperature: LLM temperature
        target_tool: If provided, only include this specific tool (improves reliability)
        tools_override: Pre-built tool schemas to use instead of registry lookup

    Returns:
        Dict with response, tools_executed, and tool_results
    """
    # Check if LLM supports tool calling
    if not hasattr(llm, "chat_with_tools"):
        logger.warning("LLM does not support tool calling, using regular chat")
        result = llm.chat(messages=messages, max_tokens=max_tokens, temperature=temperature)
        return {
            "response": _strip_tool_xml(result.get("response", "")),
            "tools_executed": [],
            "tool_results": {},
            "llm_meta": _finalize_llm_meta(_merge_llm_meta(None, _extract_llm_meta(result))),
        }

    # If caller provided pre-built tool schemas, use them directly
    if tools_override is not None:
        tools = tools_override
        logger.info("Tool executor: using %d override tools", len(tools))
    elif target_tool:
        # Use registry to resolve alias to actual tool name
        tool_name = tool_registry.resolve_alias(target_tool)

        if not tool_name:
            # Fallback: try adding common prefixes
            known_prefixes = ("get_", "set_", "list_", "check_", "book_", "cancel_", "reschedule_")
            for prefix in known_prefixes:
                candidate = f"{prefix}{target_tool}"
                if tool_registry.get(candidate):
                    tool_name = candidate
                    break

        if not tool_name:
            # Last resort: use as-is
            tool_name = target_tool

        tools = tool_registry.get_tool_schemas_filtered([tool_name])
        if tools:
            logger.info("Tool executor: using target tool '%s'", tool_name)
        else:
            logger.warning("Target tool '%s' not found, falling back to priority tools", tool_name)
            target_tool = None  # Fall through to priority tools

    if tools_override is None and not target_tool:
        # Use registry to get priority tool schemas
        tools = tool_registry.get_tool_schemas_filtered(PRIORITY_TOOL_NAMES)
        if not tools:
            tools = tool_registry.get_tool_schemas()
        logger.info("Tool executor: %d tools available", len(tools))

    logger.info("Tool names: %s", [t.get("function", {}).get("name") for t in tools])

    current_messages = list(messages)
    tool_results = {}
    last_response = ""
    llm_meta_aggregate: dict[str, Any] | None = None

    for iteration in range(MAX_TOOL_ITERATIONS):
        logger.info("Tool calling iteration %d", iteration + 1)

        result = llm.chat_with_tools(
            messages=current_messages,
            tools=tools,
            max_tokens=max_tokens,
            temperature=temperature,
        )
        llm_meta_aggregate = _merge_llm_meta(llm_meta_aggregate, _extract_llm_meta(result))

        last_response = result.get("response", "")
        tool_calls = result.get("tool_calls", [])
        logger.info("LLM response: len=%d, tool_calls=%d, response='%s'",
                   len(last_response), len(tool_calls), last_response)

        # If no structured tool calls, try parsing text-based calls
        if not tool_calls and last_response:
            tool_calls = parse_text_tool_calls(last_response)
            if tool_calls:
                logger.info("Parsed %d text-based tool call(s)", len(tool_calls))

        if not tool_calls:
            # If LLM returned empty but we have tool results, use tool result as response
            final_response = _strip_tool_xml(last_response)
            if not final_response and tool_results:
                # Use the last tool's result as the response
                final_response = list(tool_results.values())[-1]
                logger.info("LLM returned empty, using tool result as response: %s", final_response)
            elif not final_response:
                logger.warning("LLM returned empty response with no tool results")
                final_response = EMPTY_RESPONSE_FALLBACK
            else:
                logger.info("No tool calls from LLM, returning response directly")
            return {
                "response": final_response,
                "tools_executed": list(tool_results.keys()),
                "tool_results": tool_results,
                "llm_meta": _finalize_llm_meta(llm_meta_aggregate),
            }

        logger.info("LLM requested %d tool call(s)", len(tool_calls))

        # Add assistant message with tool calls
        current_messages.append(Message(
            role="assistant",
            content=last_response or "",
            tool_calls=tool_calls,
        ))

        # Process each tool call
        for call in tool_calls:
            func = call.get("function", {})
            tool_name = func.get("name", "")
            tool_call_id = call.get("id")
            args = func.get("arguments", {})

            # Parse arguments if string
            if isinstance(args, str):
                try:
                    args = json.loads(args)
                except json.JSONDecodeError:
                    logger.warning("Failed to parse tool args: %s", args)
                    args = {}

            logger.info("Executing tool: %s with args: %s", tool_name, args)

            # Execute tool with per-tool timeout
            from ..config import settings
            tool_timeout = settings.voice.tool_execution_timeout
            try:
                tool_result = await asyncio.wait_for(
                    tool_registry.execute(tool_name, args),
                    timeout=tool_timeout,
                )
            except asyncio.TimeoutError:
                logger.error("Tool %s timed out after %.1fs", tool_name, tool_timeout)
                tool_result = ToolResult(
                    success=False,
                    error="TOOL_TIMEOUT",
                    message=f"Tool {tool_name} timed out after {tool_timeout:.0f}s",
                )
            tool_results[tool_name] = tool_result.message

            # Add tool result to messages
            result_content = json.dumps({
                "name": tool_name,
                "success": tool_result.success,
                "message": tool_result.message,
                "data": tool_result.data,
                "error": tool_result.error,
            }, default=str)
            current_messages.append(Message(
                role="tool",
                content=result_content,
                tool_call_id=tool_call_id,
            ))

            logger.info(
                "Tool %s result: success=%s, message=%s",
                tool_name,
                tool_result.success,
                tool_result.message[:50] if tool_result.message else "",
            )

    # Max iterations reached -- strip any leftover tool XML from response
    logger.warning("Max tool iterations (%d) reached", MAX_TOOL_ITERATIONS)
    final_response = _strip_tool_xml(last_response)
    if not final_response and tool_results:
        final_response = list(tool_results.values())[-1]
        logger.info("Using tool result as fallback response: %s", final_response)
    return {
        "response": final_response,
        "tools_executed": list(tool_results.keys()),
        "tool_results": tool_results,
        "llm_meta": _finalize_llm_meta(llm_meta_aggregate),
    }
